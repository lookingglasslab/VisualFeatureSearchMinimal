{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interactive Visual Feature Search: Local Demo\n",
    "\n",
    "We provide a small-scale demo (without caching) of Visual Feature Search for the reader to use. We use a MobileNet_v2 model trained on ImageNet for this visualization, and we provide ten queries from the ImageNet test set in the `test_imgs` directory. \n",
    "\n",
    "We choose to provide a small-scale notebook to be run locally (on a CPU only) as this is the easiest way to share a demo that is (1) anonymous and (2) able to run on any computer.\n",
    "\n",
    "###  Setup\n",
    "First, it is required to download the ImageNet validation set for this demo. We use a subset of the validation set (1,000 images) as our searchable database for visual feature search. To get the dataset:\n",
    "1. Go to image-net.org and login or signup for access.\n",
    "2. Go to the following URL: https://image-net.org/challenges/LSVRC/2012/2012-downloads.php\n",
    "3. Download \"ILSVRC2012_img_val.tar\" file from the link titled \"Validation images (all tasks)\" under the \"Images\" header (The file size should be 6.3Gb).\n",
    "4. Extract the .tar file, and set IMAGENET_VAL_DIR below to the directory containing the extracted images.\n",
    "\n",
    "For running our demo, we recommend using anaconda/miniconda to create a temporary environment. To set everything up, complete the following steps:\n",
    "1. Run `conda create -n tmp-vfs`\n",
    "2. Run `conda activate tmp-vfs`, followed by `conda install jupyter`\n",
    "3. Start a Jupyter notebook server by running `jupyter notebook`. **NOTE:** other notebook environments, such as Jupyter Lab, may not work with our interactive widget! We have only tested our library for Google Colab and `jupyter notebook`.\n",
    "4. Open this notebook file in the Jupyter Notebook, and run the following code blocks to install & import components.\n",
    "5. Run the remaining code blocks to create the interactive widget and search for similar regions in a small-scale example by using MobileNet_v2 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGENET_VAL_DIR = 'path/to/val/images' # TODO: replace me\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import requests\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from lucent.optvis.render import ModuleHook\n",
    "\n",
    "from vissearch import widgets, util, data\n",
    "from vissearch.searchtool import LiveSearchTool, get_crop_rect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up model and search tool\n",
    "Search for similar regions in activations from the last bottleneck layer in MobileNetV2. The following code may take several minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_DATASET_SIZE = 1000\n",
    "\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "def model_layer(X):\n",
    "    hook = ModuleHook(model.features[17])\n",
    "    model(X)\n",
    "    hook.close()\n",
    "    return hook.features\n",
    "\n",
    "CONV5_FEATURE_SIZE = 7 # row/column length for the layer of interest\n",
    "\n",
    "imagenet_dataset = data.SimpleDataset(IMAGENET_VAL_DIR, return_idxs=False)\n",
    "# only search with a subset of images\n",
    "imagenet_dataset._all_images = imagenet_dataset._all_images[:SEARCH_DATASET_SIZE] \n",
    "\n",
    "device = torch.device('cpu')\n",
    "search_tool = LiveSearchTool(model_layer, device, imagenet_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up region selection UI\n",
    "\n",
    "**Instructions:** In the region selection UI below, select a query image. Then, to highlight a region, click on the image, holding down the mouse press, and drag your cursor to highlight a region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files = sorted(os.listdir('test_imgs'))\n",
    "\n",
    "query_model_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "query_imgs = [Image.open('test_imgs/' + file_name) for file_name in test_files]\n",
    "query_vis_imgs = [data.vis_transform(img) for img in query_imgs]\n",
    "# convert images to Data URLs so we can pass them into the HTML widget\n",
    "query_img_urls = [util.image_to_durl(img) for img in query_vis_imgs]\n",
    "\n",
    "highlight_data = None\n",
    "highlight_index = None\n",
    "def highlight_callback(user_data):\n",
    "    global highlight_data, highlight_index\n",
    "    highlight_data = user_data[:-2]\n",
    "    highlight_index = user_data[-1]\n",
    "util.create_callback('highlight_callback', highlight_callback)\n",
    "\n",
    "widgets.MultiHighlightWidget(all_urls=query_img_urls, callback_name='highlight_callback')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show selected region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert highlight_data is not None, \"Use the widget to highlight an image region\"\n",
    "\n",
    "# overlay the mask onto the user's selected image\n",
    "curr_img = query_vis_imgs[int(highlight_index)]\n",
    "\n",
    "mask = util.durl_to_image(highlight_data)\n",
    "mask_arr = np.asarray(mask)[:,:,3] / 256 # take just the alpha channel\n",
    "\n",
    "curr_mask_overlay = util.mask_overlay(curr_img, x=0, y=0, mask_size=224, mask=mask_arr, alpha=0.5, beta=0.4)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 3))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.axis('off')\n",
    "plt.imshow(curr_img, cmap='gray')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.axis('off')\n",
    "plt.imshow(mask_arr, cmap='gray')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.axis('off')\n",
    "_ = plt.imshow(curr_mask_overlay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute activations and find similar regions\n",
    "This code should only take a few seconds to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input selected image into model\n",
    "curr_img_tensor = data.net_transform(query_imgs[int(highlight_index)])\n",
    "search_tool.set_input_image(curr_img_tensor)\n",
    "\n",
    "# assemble masks\n",
    "donsample_transform = transforms.Resize(CONV5_FEATURE_SIZE)\n",
    "small_mask = donsample_transform(mask)\n",
    "small_mask_arr = np.asarray(small_mask)[:,:,3] / 255\n",
    "\n",
    "# compute the similarities\n",
    "print('Loading Results...')\n",
    "sims, xs, ys = search_tool.compute(small_mask_arr)\n",
    "image_order = torch.argsort(sims, descending=True)\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display most similar regions in activation space\n",
    "\n",
    "These highlighted regions share the most similar activations after the last bottleneck in MobileNetV2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize results\n",
    "# set up the figure\n",
    "DISPLAY_NUM = 6\n",
    "\n",
    "HEIGHT = 5\n",
    "WIDTH = 15\n",
    "plt.figure(figsize=(WIDTH, HEIGHT))\n",
    "\n",
    "# show the query region on the left-hand side\n",
    "ax = plt.subplot(1, DISPLAY_NUM, 1)\n",
    "plt.axis('off')\n",
    "plt.imshow(curr_mask_overlay)\n",
    "\n",
    "# draw a border between query and results\n",
    "ax.add_line(matplotlib.lines.Line2D([245,245], [0,224], lw=4, color='black')).set_clip_on(False)\n",
    "\n",
    "\n",
    "for i in range(DISPLAY_NUM-1):\n",
    "  idx = image_order[i]\n",
    "  curr_img_out = util.mask_overlay(imagenet_dataset.get_vis_image(idx), \n",
    "                                   x=xs[idx], \n",
    "                                   y=ys[idx], \n",
    "                                   mask_size=CONV5_FEATURE_SIZE, \n",
    "                                   mask=util.crop_mask(small_mask_arr))\n",
    "\n",
    "  plt.subplot(1, DISPLAY_NUM, i+2)\n",
    "  plt.axis('off')\n",
    "  plt.imshow(curr_img_out, cmap='gray')\n",
    "  plt.title(str(np.round(sims[idx].cpu().numpy(), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
